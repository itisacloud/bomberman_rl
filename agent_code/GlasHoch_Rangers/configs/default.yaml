AGENT_CONFIG:
  agent_name: GlasHoch_long_run_start_easy_no_imitation
  config_name: default

  # dimensions (change only when features change)
  state_dim: [7, 9, 9]
  action_dim: 6
  curr_step: 0

  # training
  batch_size: 32
  memory_size: 10000
  burnin: 1000
  learn_evry: 1
  sync_evry: 1000
  gamma: 0.95

  # Training learning rates etc:
  optimizer: AdamW

  learning_rate: 0.00025
  lr_scheduler_step: 1
  lr_scheduler_gamma: 0.999995
  lr_scheduler_min: 0.0001
  loss_fn: MSE
  lr_scheduler: True


  # Info
  draw_plot: True
  mode_plot: static # "static" or "dynamic"
  draw_plot_every: 100000
  debuggin: True
  debuggin_freq: 1000

  # Exploration
  exploration_method: epsilon-greedy
  exploration_rate: 0.15
  exploration_rate_decay: 0.999998
  exploration_rate_min: 0.075

  # Imitation Learning
  imitation_learning: False
  imitation_learning_rate: 1
  imitation_learning_decay: 0.999995
  imitation_learning_min: 0.1
  imitation_learning_expert: rule_based_agent
  imitation_learning_cutoff: 100000

  # checkpointing model
  load: True
  load_path: /models/GlasHoch_long_runer_2nd_slightly_higher_lr_start_easy_no_imitation_default_1000000.pth

  save_every: 10000

REWARD_CONFIG:
  MOVED_LEFT: 0.1  # Normalize rewards to be within a reasonable range
  MOVED_RIGHT: 0.1
  MOVED_UP: 0.1
  MOVED_DOWN: 0.1
  WAITED: -0.2  # Negative rewards are also normalized
  INVALID_ACTION: -0.5
  BOMB_DROPPED: 0.2
  BOMB_EXPLODED: 0.0  # No normalization needed for zero rewards
  CRATE_DESTROYED: 1.0  # Adjusted for meaningful scale
  COIN_FOUND: 1.0
  COIN_COLLECTED: 2.0
  KILLED_OPPONENT: 5.0
  KILLED_SELF: -2.0
  GOT_KILLED: -1.0
  MOVED_TOWARDS_COIN_CLUSTER: 0.2
  MOVED_TOWARDS_ENEMY: 0.1
  TRAPPED_ENEMY: 0.5
  BOMB_NEAR_CRATE: 1.0
  BOMB_NEAR_ENEMY: 3.0
  OPPONENT_ELIMINATED: 0.1
  SURVIVED_ROUND: 1.0
  ALREADY_VISITED: -0.005  # Scaled down for small impact
  MOVED_OUT_OF_DANGER: 0.1
  EXPERT_ACTION: 1.0
